# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {
    "clients.baml": '// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\n// Using the new OpenAI Responses API for enhanced formatting\nclient<llm> CustomGPT5 {\n  provider openai-responses\n  options {\n    model "gpt-5"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT5Mini {\n  provider openai-responses\n  retry_policy Exponential\n  options {\n    model "gpt-5-mini"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Openai with chat completion\nclient<llm> CustomGPT5Chat {\n  provider openai\n  options {\n    model "gpt-5"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\n// Latest Anthropic Claude 4 models\nclient<llm> CustomOpus4 {\n  provider anthropic\n  options {\n    model "claude-opus-4-1-20250805"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet4 {\n  provider anthropic\n  options {\n    model "claude-sonnet-4-20250514"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model "claude-3-5-haiku-20241022"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// Google AI client for Sagebase (gemini-2.0-flash)\nclient<llm> Gemini2Flash {\n  provider google-ai\n  retry_policy Exponential\n  options {\n    model "gemini-2.0-flash"\n    api_key env.GOOGLE_API_KEY\n    temperature 0.0\n  }\n}\n\n// Example AWS Bedrock client (uncomment to use)\n// client<llm> CustomBedrock {\n//   provider aws-bedrock\n//   options {\n//     model "anthropic.claude-sonnet-4-20250514-v1:0"\n//     region "us-east-1"\n//     // AWS credentials are auto-detected from env vars\n//   }\n// }\n\n// Example Azure OpenAI client (uncomment to use)\n// client<llm> CustomAzure {\n//   provider azure-openai\n//   options {\n//     model "gpt-5"\n//     api_key env.AZURE_OPENAI_API_KEY\n//     base_url "https://MY_RESOURCE_NAME.openai.azure.com/openai/deployments/MY_DEPLOYMENT_ID"\n//     api_version "2024-10-01-preview"\n//   }\n// }\n\n// Example Vertex AI client (uncomment to use)\n// client<llm> CustomVertex {\n//   provider vertex-ai\n//   options {\n//     model "gemini-2.5-pro"\n//     location "us-central1"\n//     // Uses Google Cloud Application Default Credentials\n//   }\n// }\n\n// Example Ollama client for local models (uncomment to use)\n// client<llm> CustomOllama {\n//   provider openai-generic\n//   options {\n//     base_url "http://localhost:11434/v1"\n//     model "llama4"\n//     default_role "user" // Most local models prefer the user role\n//     // No API key needed for local Ollama\n//   }\n// }\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT5Mini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT5Mini, CustomGPT5]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}',
    "generators.baml": '// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"\n    output_type "python/pydantic"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir "../"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version "0.214.0"\n\n    // Valid values: "sync", "async"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n',
    "member_extraction.baml": '// Conference Member Extraction for Sagebase\n// 会議体メンバー抽出用のBAML定義\n\n// 抽出されたメンバー情報の型定義\nclass ExtractedMember {\n    name string @description("議員名（フルネーム）")\n    role string? @description("役職（議長、副議長、委員長、委員など）")\n    party_name string? @description("所属政党名")\n    additional_info string? @description("その他の情報")\n}\n\n// メンバー抽出関数\nfunction ExtractMembers(\n    html: string,\n    conference_name: string\n) -> ExtractedMember[] {\n    client Gemini2Flash\n    prompt #"\n        以下のHTMLから{{ conference_name }}の議員メンバー情報を抽出してください。\n\n        重要: このページに複数の委員会や議会の情報が含まれている場合、\n        必ず「{{ conference_name }}」に所属する議員のみを抽出してください。\n        他の委員会や議会のメンバーは抽出しないでください。\n\n        HTMLコンテンツ:\n        {{ html }}\n\n        抽出する情報:\n        1. 議員名（フルネーム）\n        2. 役職（議長、副議長、委員長、副委員長、委員など）\n        3. 所属政党名（わかる場合）\n        4. その他の重要な情報\n\n        注意事項:\n        - 議員名は姓名を正確に抽出してください\n        - 敬称（議員、氏、さん、様、先生など）は除外してください\n        - 役職がない場合は「委員」としてください\n        - 所属政党が明記されていない場合はnullとしてください\n        - 複数の役職がある場合は主要な役職を選択してください\n        - 必ず指定された「{{ conference_name }}」に関連する議員のみを抽出し、\n          他の委員会や議会のメンバーは含めないでください\n    "#\n}\n',
    "resume.baml": '// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // You can also use custom LLM params with a custom client name from clients.baml like "client CustomGPT5" or "client CustomSonnet4"\n  client "openai-responses/gpt-5-mini" // Set OPENAI_API_KEY to use this client.\n  prompt #"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  "#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    "#\n  }\n}\n',
}


def get_baml_files():
    return _file_map
