"""HTML fetcher for party member pages with pagination support"""

import asyncio
import logging
from types import TracebackType
from typing import Any

from playwright.async_api import Browser, BrowserContext, Page, async_playwright

from src.infrastructure.config.settings import get_settings

from .models import WebPageContent


logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)  # Ensure INFO level logs are output


class PartyMemberPageFetcher:
    """æ”¿å…šã®è­°å“¡ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰"""

    def __init__(self, party_id: int | None = None, proc_logger: Any = None):
        self.browser: Browser | None = None
        self.context: BrowserContext | None = None
        self.settings = get_settings()
        self.party_id = party_id
        self.proc_logger = proc_logger
        # ProcessingLogger is now optional (removed with legacy Streamlit code)
        # If proc_logger is not provided, logging will use standard Python logging only
        if party_id is not None:
            self.log_key = party_id

    async def __aenter__(self):
        try:
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    "--no-sandbox",
                    "--disable-setuid-sandbox",
                    "--disable-dev-shm-usage",
                ],
            )
            self.context = await self.browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/120.0.0.0 Safari/537.36"
                )
            )
            return self
        except Exception as e:
            logger.error(f"Failed to initialize browser: {e}")
            raise

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: TracebackType | None,
    ) -> None:
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
        except Exception:
            # çµ‚äº†æ™‚ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
            pass

    async def fetch_all_pages(
        self, start_url: str, max_pages: int = 20
    ) -> list[WebPageContent]:
        """
        è­°å“¡ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰

        Args:
            start_url: é–‹å§‹URL
            max_pages: æœ€å¤§ãƒšãƒ¼ã‚¸æ•°

        Returns:
            List[WebPageContent]: å–å¾—ã—ãŸãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒªã‚¹ãƒˆ
        """
        pages_content: list[WebPageContent] = []
        visited_urls: set[str] = set()

        if not self.context:
            raise RuntimeError("Browser context not initialized")

        page = await self.context.new_page()
        if self.proc_logger:
            self.proc_logger.add_log(self.log_key, "ğŸ¬ fetch_all_pageså‡¦ç†é–‹å§‹", "info")
        try:
            # æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            logger.info(f"Fetching initial page: {start_url}")
            if self.proc_logger:
                self.proc_logger.add_log(
                    self.log_key, f"ğŸ“– æœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {start_url}", "info"
                )
            try:
                # ã¾ãšã¯domcontentloadedã§é«˜é€Ÿã«èª­ã¿è¾¼ã¿
                if self.proc_logger:
                    timeout_sec = self.settings.page_load_timeout
                    self.proc_logger.add_log(
                        self.log_key,
                        f"â³ page.gotoé–‹å§‹ (timeout={timeout_sec}ç§’)",
                        "info",
                    )
                await page.goto(
                    start_url,
                    wait_until="domcontentloaded",
                    timeout=self.settings.page_load_timeout * 1000,
                )
                if self.proc_logger:
                    self.proc_logger.add_log(
                        self.log_key, "âœ… page.gotoå®Œäº† (domcontentloaded)", "success"
                    )
                # ãã®å¾Œã€networkidleã‚’çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§è©¦ã™
                try:
                    if self.proc_logger:
                        self.proc_logger.add_log(
                            self.log_key, "â³ networkidleå¾…æ©Ÿä¸­ (5ç§’)", "info"
                        )
                    await page.wait_for_load_state(
                        "networkidle",
                        timeout=5000,  # 5ç§’ã®ã¿å¾…ã¤
                    )
                    if self.proc_logger:
                        self.proc_logger.add_log(
                            self.log_key, "âœ… networkidleå®Œäº†", "success"
                        )
                except Exception:
                    # networkidleãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¦ã‚‚ç¶šè¡Œ
                    logger.debug("Network idle timeout, but continuing")
                    if self.proc_logger:
                        self.proc_logger.add_log(
                            self.log_key,
                            "â„¹ï¸ networkidleã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (ç¶šè¡Œã—ã¾ã™)",
                            "info",
                        )
            except Exception as e:
                logger.warning(f"Initial page load with domcontentloaded failed: {e}")
                if self.proc_logger:
                    self.proc_logger.add_log(
                        self.log_key,
                        f"âš ï¸ ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ä¸­ï¼‰: {str(e)[:100]}",
                        "warning",
                    )
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: loadã‚¤ãƒ™ãƒ³ãƒˆã¾ã§å¾…ã¤
                if self.proc_logger:
                    self.proc_logger.add_log(
                        self.log_key,
                        "ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: loadã‚¤ãƒ™ãƒ³ãƒˆã§å†è©¦è¡Œ",
                        "warning",
                    )
                await page.goto(
                    start_url,
                    wait_until="load",
                    timeout=self.settings.page_load_timeout * 1000,
                )
                if self.proc_logger:
                    self.proc_logger.add_log(
                        self.log_key, "âœ… page.gotoå®Œäº† (loadã‚¤ãƒ™ãƒ³ãƒˆ)", "success"
                    )

            if self.proc_logger:
                self.proc_logger.add_log(
                    self.log_key, "â³ å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ (2ç§’)", "info"
                )
            await asyncio.sleep(2)  # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦é…å»¶èª­ã¿è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ­ãƒ¼ãƒ‰
            if self.proc_logger:
                self.proc_logger.add_log(
                    self.log_key,
                    "ğŸ“œ ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’èª­ã¿è¾¼ã¿ä¸­...",
                    "info",
                )
            for i in range(3):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(1)
                if self.proc_logger and i == 2:
                    self.proc_logger.add_log(
                        self.log_key, "âœ… ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†", "success"
                    )

            if self.proc_logger:
                self.proc_logger.add_log(
                    self.log_key, "âœ… ãƒšãƒ¼ã‚¸ã®åˆæœŸèª­ã¿è¾¼ã¿å®Œäº†", "success"
                )

            current_page_num = 1

            while current_page_num <= max_pages:
                # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                current_url = page.url
                if current_url in visited_urls:
                    logger.info("Already visited this URL, stopping pagination")
                    if self.proc_logger:
                        self.proc_logger.add_log(
                            self.log_key,
                            "â„¹ï¸ åŒã˜URLãŒæ¤œå‡ºã•ã‚ŒãŸãŸã‚ã€ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†",
                            "info",
                        )
                    break

                visited_urls.add(current_url)
                content = await page.content()

                pages_content.append(
                    WebPageContent(
                        url=current_url,
                        html_content=content,
                        page_number=current_page_num,
                    )
                )

                logger.info(f"Fetched page {current_page_num}: {current_url}")
                if self.proc_logger:
                    self.proc_logger.add_log(
                        self.log_key,
                        f"ğŸ“„ ãƒšãƒ¼ã‚¸{current_page_num}ã‚’å–å¾—: {current_url}",
                        "info",
                    )

                # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                next_link = await self._find_next_page_link(page)

                if not next_link:
                    logger.info("No more pages found")
                    if self.proc_logger:
                        self.proc_logger.add_log(
                            self.log_key, "â„¹ï¸ ã“ã‚Œä»¥ä¸Šã®ãƒšãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“", "info"
                        )
                    break

                # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
                try:
                    logger.info("Attempting to click next page link")
                    if self.proc_logger:
                        self.proc_logger.add_log(
                            self.log_key,
                            f"â¡ï¸ ãƒšãƒ¼ã‚¸{current_page_num + 1}ã¸ç§»å‹•ä¸­...",
                            "info",
                        )
                    await next_link.click()
                    # domcontentloadedã‚’å¾…ã¤
                    await page.wait_for_load_state(
                        "domcontentloaded",
                        timeout=self.settings.page_load_timeout * 1000,
                    )
                    # networkidleã¯çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§è©¦ã™
                    try:
                        await page.wait_for_load_state("networkidle", timeout=3000)
                    except Exception:
                        logger.debug("Network idle timeout on pagination, continuing")
                    await asyncio.sleep(2)
                except Exception as e:
                    logger.warning(f"Failed to navigate to next page: {e}")
                    if self.proc_logger:
                        self.proc_logger.add_log(
                            self.log_key,
                            f"âš ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸ã®ç§»å‹•å¤±æ•—: {str(e)[:100]}",
                            "warning",
                        )
                    break

                current_page_num += 1

            if self.proc_logger and pages_content:
                self.proc_logger.add_log(
                    self.log_key,
                    f"âœ… å…¨{len(pages_content)}ãƒšãƒ¼ã‚¸ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸ",
                    "success",
                )
            return pages_content

        except Exception as e:
            logger.error(f"Error during page fetching from {start_url}: {e}")
            import traceback

            traceback_str = traceback.format_exc()
            logger.error(f"Traceback: {traceback_str}")

            # Streamlitã«ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º
            if self.proc_logger:
                self.proc_logger.add_log(
                    self.log_key, f"âŒ ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", "error"
                )
                # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã®ä¸€éƒ¨ã‚‚è¡¨ç¤º
                self.proc_logger.add_log(
                    self.log_key,
                    f"ğŸ” è©³ç´°: {traceback_str[:500]}",
                    "error",
                )
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å¯¾å‡¦æ³•ã‚‚è¡¨ç¤º
                if "Timeout" in str(e):
                    self.proc_logger.add_log(
                        self.log_key,
                        "ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ã‚µã‚¤ãƒˆãŒé…ã„ã§ã™ã€‚æ™‚é–“ã‚’ãŠã„ã¦å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
                        "info",
                    )

            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã€å–å¾—æ¸ˆã¿ã®ãƒšãƒ¼ã‚¸ã¯è¿”ã™
            return pages_content if pages_content else []
        finally:
            await page.close()

    async def _find_next_page_link(self, page: Page):
        """æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™"""
        # ä¸€èˆ¬çš„ãªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³
        next_patterns = [
            'a:has-text("æ¬¡ã¸")',
            'a:has-text("æ¬¡")',
            'a:has-text("Next")',
            'a:has-text(">")',
            'a:has-text("Â»")',
            'a[rel="next"]',
            ".pagination a.next",
            ".pager a.next",
            "a.page-next",
            "li.next a",
        ]

        for pattern in next_patterns:
            try:
                element = await page.query_selector(pattern)
                if element and await element.is_visible():
                    # ãƒªãƒ³ã‚¯ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ãªã„ã‹ç¢ºèª
                    is_disabled = (
                        await element.get_attribute("disabled")
                        or await element.get_attribute("aria-disabled") == "true"
                        or "disabled" in (await element.get_attribute("class") or "")
                    )

                    if not is_disabled:
                        return element
            except Exception:
                continue

        # æ•°å­—ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç¾åœ¨ã®ãƒšãƒ¼ã‚¸ç•ªå·ã®æ¬¡ï¼‰
        try:
            # ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ç‰¹å®š
            current_page_elem = await page.query_selector(
                ".pagination .active, .pager .current, .page-current"
            )
            if current_page_elem:
                current_text = await current_page_elem.text_content()
                if current_text and current_text.strip().isdigit():
                    current_num = int(current_text.strip())
                    next_num = current_num + 1

                    # æ¬¡ã®ãƒšãƒ¼ã‚¸ç•ªå·ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    next_link = await page.query_selector(f'a:has-text("{next_num}")')
                    if next_link and await next_link.is_visible():
                        return next_link
        except Exception:
            pass

        return None

    async def fetch_single_page(self, url: str) -> WebPageContent | None:
        """å˜ä¸€ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        if not self.context:
            raise RuntimeError("Browser context not initialized")

        page = await self.context.new_page()
        try:
            logger.info(f"Fetching page: {url}")
            try:
                # ã¾ãšã¯domcontentloadedã§é«˜é€Ÿã«èª­ã¿è¾¼ã¿
                await page.goto(
                    url,
                    wait_until="domcontentloaded",
                    timeout=self.settings.page_load_timeout * 1000,
                )
                # networkidleã¯çŸ­ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§è©¦ã™
                try:
                    await page.wait_for_load_state("networkidle", timeout=5000)
                except Exception:
                    logger.debug("Network idle timeout, but continuing")
            except Exception as e:
                logger.warning(f"Page load with domcontentloaded failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: loadã‚¤ãƒ™ãƒ³ãƒˆã¾ã§å¾…ã¤
                await page.goto(
                    url,
                    wait_until="load",
                    timeout=self.settings.page_load_timeout * 1000,
                )
            await asyncio.sleep(2)

            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦é…å»¶èª­ã¿è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ­ãƒ¼ãƒ‰
            for _ in range(3):
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(1)

            content = await page.content()
            return WebPageContent(url=url, html_content=content, page_number=1)

        except Exception as e:
            logger.error(f"Error fetching page {url}: {e}")
            return None
        finally:
            await page.close()
