"""Web scraper service implementation using Playwright."""

from typing import Any

from src.domain.services.interfaces.web_scraper_service import IWebScraperService


class PlaywrightScraperService(IWebScraperService):
    """Playwright-based implementation of web scraper."""

    def __init__(self, headless: bool = True, llm_service: Any | None = None):
        """Initialize the PlaywrightScraperService.

        Args:
            headless: Whether to run the browser in headless mode
            llm_service: Optional LLM service for content extraction.
                        If not provided, a default GeminiLLMService will be created.
        """
        self.headless = headless
        self._llm_service = llm_service
        # Initialize Playwright here

    def is_supported_url(self, url: str) -> bool:
        """Check if the URL is supported for scraping.

        Args:
            url: URL to check

        Returns:
            True if the URL is supported, False otherwise
        """
        # Support common Japanese government and council websites
        supported_domains = [
            "kaigiroku.net",
            "metro.tokyo.lg.jp",
            "pref.kyoto.lg.jp",
            "pref.osaka.lg.jp",
            "city.kyoto.lg.jp",
            "city.osaka.lg.jp",
            "shugiin.go.jp",
            "sangiin.go.jp",
        ]

        return any(domain in url for domain in supported_domains)

    async def fetch_html(self, url: str) -> str:
        """Fetch raw HTML content from a URL using Playwright.

        Args:
            url: URL to fetch

        Returns:
            Raw HTML content as string

        Raises:
            ValueError: If URL is invalid or inaccessible
        """
        import logging

        from playwright.async_api import async_playwright

        logger = logging.getLogger(__name__)

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=self.headless)
                page = await browser.new_page()

                # Navigate to the URL
                await page.goto(url, wait_until="networkidle", timeout=30000)

                # Wait for content to load
                await page.wait_for_load_state("domcontentloaded")

                # Get the HTML content
                html_content = await page.content()

                await browser.close()

                logger.debug(f"Fetched HTML from {url} ({len(html_content)} bytes)")
                return html_content

        except Exception as e:
            logger.error(f"Failed to fetch HTML from {url}: {e}")
            raise ValueError(f"Failed to fetch HTML from {url}: {e}") from e

    async def scrape_party_members(
        self, url: str, party_id: int, party_name: str | None = None
    ) -> list[dict[str, Any]]:
        """Scrape party members using Playwright with actual implementation."""
        import asyncio
        import logging

        from src.interfaces.factories.party_member_extractor_factory import (
            PartyMemberExtractorFactory,
        )
        from src.party_member_extractor.html_fetcher import PartyMemberPageFetcher

        logger = logging.getLogger(__name__)
        # ProcessingLogger removed with legacy Streamlit code
        # Now using standard Python logging only
        proc_logger = None

        # Get party name if not provided
        if party_name is None:
            # Try to get party name from database
            from sqlalchemy.orm import sessionmaker

            from src.infrastructure.config.database import get_db_engine
            from src.infrastructure.persistence.async_session_adapter import (
                AsyncSessionAdapter,
            )
            from src.infrastructure.persistence.political_party_repository_impl import (
                PoliticalPartyRepositoryImpl,
            )

            engine = get_db_engine()
            session_local = sessionmaker(bind=engine)
            session = session_local()
            async_session = AsyncSessionAdapter(session)
            party_repo = PoliticalPartyRepositoryImpl(async_session)

            try:
                party = await party_repo.get_by_id(party_id)
                if party:
                    party_name = party.name
                else:
                    party_name = "Unknown Party"
            finally:
                session.close()

        try:
            # Log the start of web scraping
            logger.info(f"ğŸŒ Webãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {url}")

            # Fetch pages with JavaScript rendering support
            fetcher = None
            try:
                fetcher = PartyMemberPageFetcher(
                    party_id=party_id, proc_logger=proc_logger
                )
                await fetcher.__aenter__()

                logger.info("ğŸ“„ JavaScriptãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
                logger.info(f"ğŸ¯ URL: {url}ã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚’å–å¾—é–‹å§‹")

                pages = await fetcher.fetch_all_pages(url, max_pages=10)

                logger.info(f"ğŸ¬ fetch_all_pageså®Œäº† - {len(pages)}ãƒšãƒ¼ã‚¸å–å¾—")

                if not pages:
                    logger.warning(f"No pages fetched from {url}")
                    logger.warning("âš ï¸ ãƒšãƒ¼ã‚¸ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    return []

                logger.info(f"âœ… {len(pages)}ãƒšãƒ¼ã‚¸å–å¾—å®Œäº†")

                # Log page URLs for debugging
                for i, page in enumerate(pages, 1):
                    logger.debug(f"  ãƒšãƒ¼ã‚¸{i}: {page.url}")

                # Extract party members using LLM
                logger.info("ğŸ¤– LLMã§æ”¿æ²»å®¶æƒ…å ±ã‚’æŠ½å‡ºä¸­...")

                extractor = PartyMemberExtractorFactory.create()
                members_list = await extractor.extract_from_pages(pages, party_name)

                # Convert to expected format
                result = []
                if members_list and members_list.members:
                    member_names = []
                    for member in members_list.members:
                        result.append(
                            {
                                "name": member.name,
                                "furigana": None,  # Not available in PartyMemberInfo
                                "district": member.electoral_district,
                                "profile_page_url": member.profile_url,
                            }
                        )
                        member_names.append(member.name)

                    logger.info(f"âœ… {len(result)}äººã®æ”¿æ²»å®¶æƒ…å ±ã‚’æŠ½å‡º")

                    # Log extracted member names for debugging
                    if member_names:
                        names_display = ", ".join(member_names[:10])
                        if len(member_names) > 10:
                            names_display += f" ... ä»–{len(member_names) - 10}äºº"
                        logger.info(f"æŠ½å‡ºã•ã‚ŒãŸè­°å“¡: {names_display}")

                return result

            finally:
                # Ensure proper cleanup
                if fetcher:
                    await fetcher.__aexit__(None, None, None)
                    # Give asyncio time to clean up
                    await asyncio.sleep(0.1)

        except Exception as e:
            logger.error(f"Failed to scrape party members from {url}: {e}")
            logger.error("âŒ æ”¿æ²»å®¶æŠ½å‡ºå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")

            # ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã«å¿œã˜ãŸãƒ’ãƒ³ãƒˆã‚’è¡¨ç¤º
            if "Timeout" in str(e):
                logger.info(
                    "ğŸ’¡ å¯¾å‡¦æ³•: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€‚ã‚µã‚¤ãƒˆãŒæ··é›‘ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                )
            elif "Failed to initialize" in str(e):
                logger.info(
                    "ğŸ’¡ å¯¾å‡¦æ³•: ãƒ–ãƒ©ã‚¦ã‚¶åˆæœŸåŒ–å¤±æ•—ã€‚ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )
            elif "LLM" in str(e) or "Gemini" in str(e):
                logger.info(
                    "ğŸ’¡ å¯¾å‡¦æ³•: AIå‡¦ç†ã‚¨ãƒ©ãƒ¼ã€‚APIã‚­ãƒ¼ã‚„ã‚¯ã‚©ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )

            # Return empty list on error instead of dummy data
            return []

    async def scrape_conference_members(self, url: str) -> list[dict[str, Any]]:
        """Scrape conference members using Playwright."""
        # Implementation would use Playwright to navigate and extract data
        # This is a placeholder
        return [
            {
                "name": "ä½è—¤èŠ±å­",
                "party": "â—‹â—‹å…š",
                "role": "è­°é•·",
                "profile_url": "https://example.com/member/1",
            }
        ]

    async def scrape_meeting_minutes(
        self, url: str, upload_to_gcs: bool = False
    ) -> dict[str, Any]:
        """Scrape meeting minutes using Playwright."""
        # Implementation would use Playwright to navigate and extract data
        # This is a placeholder
        return {
            "meeting_date": "2024-01-15",
            "meeting_name": "æœ¬ä¼šè­°",
            "pdf_url": "https://example.com/minutes.pdf",
            "text_content": "ä¼šè­°ã®å†…å®¹...",
            "gcs_pdf_uri": "gs://bucket/minutes.pdf" if upload_to_gcs else None,
            "gcs_text_uri": "gs://bucket/minutes.txt" if upload_to_gcs else None,
        }

    async def scrape_proposal_judges(self, url: str) -> list[dict[str, Any]]:
        """Scrape proposal voting information from website.

        Args:
            url: URL of the proposal voting results page

        Returns:
            List of voting information with name, party, and judgment
        """
        import logging

        from playwright.async_api import async_playwright

        from src.domain.services.proposal_judge_extraction_service import (
            ProposalJudgeExtractionService,
        )

        logger = logging.getLogger(__name__)

        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=self.headless)
                page = await browser.new_page()

                # Navigate to the URL
                await page.goto(url, wait_until="networkidle")

                # Wait for content to load
                await page.wait_for_load_state("domcontentloaded")

                # Get the page content
                text_content = await page.inner_text("body")

                await browser.close()

                # Use LLM to extract voting information
                # Use injected service or create default
                if self._llm_service:
                    llm_service = self._llm_service
                else:
                    from src.infrastructure.external.llm_service import GeminiLLMService

                    llm_service = GeminiLLMService()

                # Extract voting information using LLM
                import json

                from langchain_core.prompts import ChatPromptTemplate

                prompt = f"""
ä»¥ä¸‹ã®ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰è­°æ¡ˆã®è³›å¦æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒšãƒ¼ã‚¸ã®URL: {url}
ãƒšãƒ¼ã‚¸ã®å†…å®¹:
{text_content[:8000]}

ä»¥ä¸‹ã®å½¢å¼ã®JSONé…åˆ—ã¨ã—ã¦è¿”ã—ã¦ãã ã•ã„ã€‚ä¼šæ´¾åã‚„è­°å“¡å›£åãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®å˜ä½ã§æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
[
    {{"name": "ä¼šæ´¾åã¾ãŸã¯è­°å“¡å", "party": "æ‰€å±æ”¿å…šï¼ˆã‚ã‹ã‚‹å ´åˆï¼‰",
      "judgment": "è³›æˆã¾ãŸã¯åå¯¾ã¾ãŸã¯æ£„æ¨©ã¾ãŸã¯æ¬ å¸­"}},
    ...
]

æ³¨æ„äº‹é …:
- ä¼šæ´¾åï¼ˆä¾‹ï¼šè‡ªç”±æ°‘ä¸»å…šè­°å“¡å›£ãªã©ï¼‰ã§è³›å¦ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¼šæ´¾åã‚’nameã«è¨˜è¼‰
- å€‹åˆ¥ã®è­°å“¡åãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€è­°å“¡åã‚’nameã«è¨˜è¼‰
- è³›æˆã¯ã€Œè³›æˆã€ã€åå¯¾ã¯ã€Œåå¯¾ã€ã€æ£„æ¨©ã¯ã€Œæ£„æ¨©ã€ã€æ¬ å¸­ã¯ã€Œæ¬ å¸­ã€ã¨è¨˜è¼‰
- æ•¬ç§°ï¼ˆè­°å“¡ã€å›ã€ã•ã‚“ç­‰ï¼‰ã¯é™¤å»
- JSONã®ã¿ã‚’è¿”ã—ã€ä»–ã®èª¬æ˜æ–‡ã¯å«ã‚ãªã„
"""

                try:
                    # Use the LLM directly for extraction
                    if hasattr(llm_service, "get_llm"):
                        llm = llm_service.get_llm()  # type: ignore
                    elif hasattr(llm_service, "_llm"):
                        llm = llm_service._llm  # type: ignore
                    else:
                        llm = llm_service.get_structured_llm(dict)

                    # Create prompt template
                    prompt_template = ChatPromptTemplate.from_template("{text}")
                    chain = prompt_template | llm

                    # Get response
                    response = await chain.ainvoke({"text": prompt})

                    # Parse response
                    if hasattr(response, "content"):
                        response_text = response.content
                    else:
                        response_text = str(response)

                    # Ensure response_text is a string
                    if not isinstance(response_text, str):
                        response_text = str(response_text)

                    # Try to extract JSON from the response
                    # Remove markdown code blocks if present
                    response_text = response_text.strip()
                    if response_text.startswith("```json"):
                        response_text = response_text[7:]
                    if response_text.startswith("```"):
                        response_text = response_text[3:]
                    if response_text.endswith("```"):
                        response_text = response_text[:-3]

                    response_text = response_text.strip()

                    # Parse JSON
                    judges_data = json.loads(response_text)

                    count = len(judges_data) if isinstance(judges_data, list) else 0
                    logger.info(f"Successfully extracted {count} judges from {url}")

                except Exception as parse_error:
                    logger.warning(
                        f"Failed to parse LLM response as JSON: {parse_error}"
                    )
                    # Fallback: try to parse text content
                    judges_data = (
                        ProposalJudgeExtractionService.parse_voting_result_text(
                            text_content
                        )
                    )

                # Process the extracted data
                if isinstance(judges_data, list):
                    # Normalize the data using domain service
                    normalized_judges = []
                    for judge in judges_data:
                        judgment_text = judge.get("judgment", "")
                        normalized_judgment, is_known = (
                            ProposalJudgeExtractionService.normalize_judgment_type(
                                judgment_text
                            )
                        )

                        # Log unknown judgment types
                        if not is_known:
                            logger.warning(
                                f"Unknown judgment type: {judgment_text}, "
                                f"defaulting to APPROVE"
                            )

                        normalized_judges.append(
                            {
                                "name": (
                                    ProposalJudgeExtractionService.normalize_politician_name(
                                        judge.get("name", "")
                                    )
                                ),
                                "party": judge.get("party"),
                                "judgment": normalized_judgment,
                            }
                        )
                    return normalized_judges

                # If not a list, try text parsing
                return ProposalJudgeExtractionService.parse_voting_result_text(
                    text_content
                )

        except Exception as e:
            logger.error(f"Failed to scrape proposal judges from {url}: {e}")
            return []
